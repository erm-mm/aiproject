{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da47f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# находим корень проекта a4s-eval \n",
    "from pathlib import Path\n",
    "\n",
    "def get_project_root() -> Path:\n",
    "    \"\"\"\n",
    "    Возвращает корень проекта a4s-eval вне зависимости от того, \n",
    "    где запускается код: из тестов, из a4s_eval, из ноутбуков.\n",
    "    \"\"\"\n",
    "    current = Path.cwd().resolve()\n",
    "\n",
    "    # Ищем директорию, где находится папка a4s_eval\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"a4s_eval\").exists():\n",
    "            return parent\n",
    "\n",
    "    # Fallback — если запускается из .py, где есть __file__\n",
    "    try:\n",
    "        file_path = Path(__file__).resolve()\n",
    "        for parent in [file_path] + list(file_path.parents):\n",
    "            if (parent / \"a4s_eval\").exists():\n",
    "                return parent\n",
    "    except NameError:\n",
    "        pass  # __file__ не существует (например, в Jupyter)\n",
    "\n",
    "    raise RuntimeError(\"Не удалось найти корень проекта a4s-eval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5376102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OUR METRICS ===\n",
      "MAE: 0.024490033253484568\n",
      "MSE: 0.0015794964938804028\n",
      "R2: 0.9980326174775115\n",
      "\n",
      "=== SKLEARN METRICS ===\n",
      "MAE: 0.024490033253484568\n",
      "MSE: 0.0015794964938804028\n",
      "R2: 0.9980326174775115\n",
      "\n",
      "=== ABS DIFFERENCES ===\n",
      "MAE diff: 0.0\n",
      "MSE diff: 0.0\n",
      "R2 diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "from tests.additional_files.train_regression_model import run\n",
    "from a4s_eval.metric_registries.prediction_metric_registry import prediction_metric_registry\n",
    "\n",
    "# Получаем реальные данные\n",
    "y_test, y_pred = run()\n",
    "\n",
    "# Наши функции из registry\n",
    "mae_func = prediction_metric_registry.get_functions()[\"mae\"]\n",
    "mse_func = prediction_metric_registry.get_functions()[\"mse\"]\n",
    "r2_func = prediction_metric_registry.get_functions()[\"r2\"]\n",
    "\n",
    "# Dummy dataset, как в тесте\n",
    "Dataset = type(\"Dataset\", (), {\"y\": y_test})\n",
    "dataset = Dataset()\n",
    "\n",
    "# Наши вычисления\n",
    "mae_ours = mae_func(None, None, dataset, y_pred)[0]\n",
    "mse_ours = mse_func(None, None, dataset, y_pred)[0]\n",
    "r2_ours = r2_func(None, None, dataset, y_pred)[0]\n",
    "\n",
    "# sklearn значения\n",
    "mae_sk = mean_absolute_error(y_test, y_pred)\n",
    "mse_sk = mean_squared_error(y_test, y_pred)\n",
    "r2_sk = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"=== OUR METRICS ===\")\n",
    "print(\"MAE:\", mae_ours)\n",
    "print(\"MSE:\", mse_ours)\n",
    "print(\"R2:\",  r2_ours)\n",
    "\n",
    "print(\"\\n=== SKLEARN METRICS ===\")\n",
    "print(\"MAE:\", mae_sk)\n",
    "print(\"MSE:\", mse_sk)\n",
    "print(\"R2:\",  r2_sk)\n",
    "\n",
    "print(\"\\n=== ABS DIFFERENCES ===\")\n",
    "print(\"MAE diff:\", abs(mae_ours - mae_sk))\n",
    "print(\"MSE diff:\", abs(mse_ours - mse_sk))\n",
    "print(\"R2 diff:\",  abs(r2_ours - r2_sk))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ea7fff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m y_true, y_pred = \u001b[43mrun\u001b[49m(n_rows=\u001b[32m1000\u001b[39m)\n\u001b[32m      4\u001b[39m value, peaks = P3E(model=\u001b[38;5;28;01mNone\u001b[39;00m, X=\u001b[38;5;28;01mNone\u001b[39;00m, dataset=\u001b[38;5;28;01mNone\u001b[39;00m, y_pred=y_pred, y_true=y_true)\n\u001b[32m      6\u001b[39m plt.figure(figsize=(\u001b[32m12\u001b[39m,\u001b[32m4\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'run' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\"1️⃣ Визуальный контроль\n",
    "Нарисуй график y_true и y_pred вместе.\n",
    "На графике отметь пики из DeDiPeak (peaks_true и peaks_pred).\n",
    "Проверь, совпадают ли пики, и насколько сильно они смещены во времени и по амплитуде.\n",
    "Если пики почти совпадают — метрика должна быть маленькой, если сильно расходятся — большая.\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_true, y_pred = run(n_rows=1000)\n",
    "value, peaks = P3E(model=None, X=None, dataset=None, y_pred=y_pred, y_true=y_true)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(y_true, label='y_true')\n",
    "plt.plot(y_pred, label='y_pred')\n",
    "plt.scatter(peaks['peaks_true'], y_true[peaks['peaks_true']], color='green', label='True peaks')\n",
    "plt.scatter(peaks['peaks_pred'], y_pred[peaks['peaks_pred']], color='red', label='Predicted peaks')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52aba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"2️⃣ Мини-примеры\n",
    "Создай очень маленький массив, где пики известны руками, и запусти метрику на нём. \n",
    "Здесь ты сам видишь, где пики и какое расстояние получилось.\n",
    "Если результат соответствует твоему ожиданию, значит функция работает корректно.\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "y_true = np.array([0, 1, 0, 0, 1, 0])\n",
    "y_pred = np.array([0, 0.9, 0, 0, 1.1, 0])\n",
    "\n",
    "value, peaks = P3E(model=None, X=None, dataset=None, y_pred=y_pred, y_true=y_true)\n",
    "print(peaks)\n",
    "print(\"DeDiPeak:\", value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3c0a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"3️⃣ Проверка логики метрики\n",
    "Посмотри на код dedipeak_metric.py.\n",
    "Убедись, что P3E считает именно расстояние между найденными пиками (peaks_true ↔ peaks_pred) с нужной нормализацией.\n",
    "Можно добавить print внутри функции для отладки: какие пики выбраны и какие расстояния считаются.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03163ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#код для проверки загрузки и очистки данных, чтобы сравнить насколько датасет уменьшился после очистки \n",
    "import pandas as pd\n",
    "\n",
    "path = \"tests/data/household_power_consumption.txt\"\n",
    "\n",
    "# Загружаем сырые данные\n",
    "df_raw = pd.read_csv(\n",
    "    path,\n",
    "    sep=\";\",\n",
    "    low_memory=False,\n",
    "    na_values=[\"?\"]\n",
    ")\n",
    "\n",
    "raw_count = len(df_raw)\n",
    "\n",
    "# Обрабатываем — создаём Datetime\n",
    "df_raw[\"Datetime\"] = pd.to_datetime(\n",
    "    df_raw[\"Date\"] + \" \" + df_raw[\"Time\"],\n",
    "    dayfirst=True,\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Удаляем NaN/NaT\n",
    "df_clean = df_raw.dropna()\n",
    "clean_count = len(df_clean)\n",
    "\n",
    "print(f\"Total rows before cleaning: {raw_count}\")\n",
    "print(f\"Rows after cleaning: {clean_count}\")\n",
    "print(f\"Rows removed: {raw_count - clean_count}\")\n",
    "print(f\"Percentage removed: {100 * (raw_count - clean_count) / raw_count:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a4s-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
